
# Configuration File for RL Bootcamp Tutorial
# Degrees of Freedom: Specifies the number of control inputs and observed states in the environment.
degrees-of-freedom: 5

# Terminal Conditions: Defines conditions under which an episode ends.
terminal-conditions:
  # Maximum number of steps allowed per episode.
  MAX-TIME: 1000

  # Whether to enforce boundary conditions on beam positions.
  boundary-conditions: true

  # Scaling factor applied to penalties when boundary conditions are violated.
  penalty-scaling: 100.0

# MPC (Model Predictive Control) Settings: Parameters for the MPC algorithm.
mpc-settings:
  # Length of the prediction horizon for MPC.
  horizon-length: 8

  # Tolerance for convergence in the MPC solver.
  tol: 1.0e-8

  # Whether to display solver output during optimization.
  disp: False

# RL (Reinforcement Learning) Settings: Parameters for the RL algorithm.
rl-settings:
  algorithm: 'PPO' # Change this to 'TRPO' to use TRPO from stable-baselines
  total_steps: 150000 # Total number of training steps for the RL agent
  evaluation_steps: 20 # Number of steps between each evaluation of the RL agent

  # PPO specific hyperparameters
  ppo:
    learning_rate: 3.0e-4 # Controls the step size of policy and value function updates
    n_steps: 2048 # Determines the amount of experience collected before each update
    batch_size: 64 # Affects the stability and speed of training
    n_epochs: 10 # Controls how many times the collected data is reused for updates
    gamma: 0.99 # Determines the importance of future rewards in decision-making
    gae_lambda: 0.95 # Trades off bias vs. variance in advantage estimation
    clip_range: 0.2 # Limits the size of policy updates to improve stability
    ent_coef: 0.0 # Encourages exploration by adding an entropy bonus to the objective
    vf_coef: 0.5 # Balances the importance of value function learning vs. policy learning
    max_grad_norm: 0.5 # Prevents excessively large gradient updates to improve stability
    use_sde: False # Whether to use generalized State Dependent Exploration
    sde_sample_freq: -1 # The frequency of sampling a new noise matrix

  # TRPO specific hyperparameters
  trpo:
    learning_rate: 1.0e-3 # Influences how quickly the value function is updated
    n_steps: 2048 # Determines the amount of experience collected before each update
    batch_size: 128 # Affects the stability and speed of value function training
    gamma: 0.99 # Controls the importance of future rewards in decision-making
    cg_max_steps: 15 # Impacts the precision of the natural gradient computation
    cg_damping: 0.1 # Helps stabilize the natural gradient computation
    line_search_shrinking_factor: 0.8 # Affects how conservatively the policy is updated
    line_search_max_iter: 10 # Limits the computational cost of finding an acceptable policy update
    n_critic_updates: 10 # Balances the learning between policy and value function
    gae_lambda: 0.95 # Trades off bias vs. variance in advantage estimation
    use_sde: False # Whether to use generalized State Dependent Exploration
    sde_sample_freq: -1 # The frequency of sampling a new noise matrix
    normalize_advantage: True # Can improve training stability across different scales of rewards
    target_kl: 0.01 # Controls how conservative the policy updates are
    sub_sampling_factor: 1 # Can reduce computation time at the cost of using less data

# Advanced Settings
# Note: Modify these settings only if necessary.
noise_setting:
  # Standard deviation of noise added to observations. Set to 'none' to disable noise.
  std_noise: none

# Initial Scaling: Scaling factor applied to initial observations to normalize state values.
init_scaling: 0.9

# Action Scale: Scaling factor applied to actions to control the magnitude of control inputs.
action_scale: 1.0

# Validation Settings: Parameters for validating the trained RL agent.
validation-settings:
  # Seeds used for reproducible validation runs.
  validation-seeds: [0, 2, 3, 4, 5, 7, 8, 11, 12, 14]

# Task Settings: Defines which task to load and its location.
task_setting:
  # Path to the file containing predefined tasks for verification.
  task_location: 'environment/tasks/verification_tasks.pkl'

  # Task number to load from the task file.
  task_nr: 0
