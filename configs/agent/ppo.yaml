_target_: stable_baselines3.PPO
policy: MlpPolicy
env: ${env}

# Learning rate for the optimizer. Controls the step size of policy and value function updates.
learning_rate: 3.0e-4

# Number of steps to run for each environment per update. Determines the amount of experience collected before each update.
n_steps: 2048

# Minibatch size for gradient updates. Affects the stability and speed of training.
batch_size: 64

# Number of epochs when optimizing the surrogate loss. Controls how many times the collected data is reused for updates.
n_epochs: 10

# Discount factor. Determines the importance of future rewards in decision-making.
gamma: 0.99

# Factor for trade-off of bias vs variance for Generalized Advantage Estimator. Trades off bias vs. variance in advantage estimation.
gae_lambda: 0.95

# Clipping parameter for the policy loss. Limits the size of policy updates to improve stability.
clip_range: 0.2

# Entropy coefficient for the loss calculation. Encourages exploration by adding an entropy bonus to the objective.
ent_coef: 0.0

# Value function coefficient for the loss calculation. Balances the importance of value function learning vs. policy learning.
vf_coef: 0.5

# The maximum value for the gradient clipping. Prevents excessively large gradient updates to improve stability.
max_grad_norm: 0.5

# Whether to use generalized State Dependent Exploration.
use_sde: False

# The frequency of sampling a new noise matrix when using gSDE.
sde_sample_freq: -1